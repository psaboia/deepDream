{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Setting up the DeepDream configuration\n",
    "\n",
    "Dictionary mapping layer names to a coefficient quantifying how mush the layer's activation contributes to the loss we will seek to maximize.\n",
    "\n",
    "_Note:_ The layer names are hardcoded in the built-in Inception V3 application.\n",
    "\n",
    "__Layers that are lower in the network contain more-local__, less-abstract\n",
    "representations and lead to dream patterns that look more geometric. \n",
    "\n",
    "__Layers that are higher up__ lead to more-recognizable visual patterns based on the most common\n",
    "objects found in ImageNet, such as dog eyes, bird feathers, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_contributions = {\n",
    "    'mixed0': 3.,    \n",
    "    'mixed1': 0.,    \n",
    "    'mixed2': 0.,\n",
    "    'mixed3': 0.,\n",
    "    'mixed4': 0.,\n",
    "    'mixed5': 0.,    \n",
    "    'mixed6': 0.,\n",
    "    'mixed7': 0.,\n",
    "    'mixed8': 0.,\n",
    "    'mixed9': 0.,\n",
    "    'mixed9_1': 0.,\n",
    "    'mixed10': 0.,    \n",
    "}\n",
    "\n",
    "# Playing with these hyperparameters will let you achieve new effects.\n",
    "step = 0.01          # Gradient ascent step size \n",
    "num_octave = 3       # Number of scales at which to run gradient ascent\n",
    "octave_scale = 1.4   # Size ratio between scales\n",
    "\n",
    "# Number of ascent steps to run at each scale\n",
    "iterations = 20\n",
    "\n",
    "# If the loss grows larger than 10, youâ€™ll interrupt the gradient-ascent process to avoid ugly artifacts.\n",
    "max_loss = 10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original image\n",
    "<img src=\"imgs/img_2.png\">\n",
    "\n",
    "#### mixed0: 3.0\n",
    "<img src=\"imgs/single_layer_contrib_value-3/mixed0_dream_at_scale_(400, 711).png\">\n",
    "\n",
    "#### mixed1: 3.0\n",
    "<img src=\"imgs/single_layer_contrib_value-3/mixed1_dream_at_scale_(400, 711).png\">\n",
    "\n",
    "#### mixed2: 3.0\n",
    "<img src=\"imgs/single_layer_contrib_value-3/mixed2_dream_at_scale_(400, 711).png\">\n",
    "\n",
    "#### mixed3: 3.0\n",
    "<img src=\"imgs/single_layer_contrib_value-3/mixed3_dream_at_scale_(400, 711).png\">\n",
    "\n",
    "#### mixed4: 3.0\n",
    "<img src=\"imgs/single_layer_contrib_value-3/mixed4_dream_at_scale_(400, 711).png\">\n",
    "\n",
    "#### mixed5: 3.0\n",
    "<img src=\"imgs/single_layer_contrib_value-3/mixed5_dream_at_scale_(400, 711).png\">\n",
    "\n",
    "#### mixed6: 3.0\n",
    "<img src=\"imgs/single_layer_contrib_value-3/mixed6_dream_at_scale_(400, 711).png\">\n",
    "\n",
    "#### mixed7: 3.0\n",
    "<img src=\"imgs/single_layer_contrib_value-3/mixed7_dream_at_scale_(400, 711).png\">\n",
    "\n",
    "#### mixed8: 3.0\n",
    "<img src=\"imgs/single_layer_contrib_value-3/mixed8_dream_at_scale_(400, 711).png\">\n",
    "\n",
    "#### mixed9_1: 3.0\n",
    "<img src=\"imgs/single_layer_contrib_value-3/mixed9_dream_at_scale_(400, 711).png\">\n",
    "\n",
    "#### mixed9: 3.0\n",
    "<img src=\"imgs/single_layer_contrib_value-3/mixed9_1_dream_at_scale_(400, 711).png\">\n",
    "\n",
    "#### mixed10: 3.0\n",
    "<img src=\"imgs/single_layer_contrib_value-3/mixed10_dream_at_scale_(400, 711).png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
